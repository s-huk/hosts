################################################################################
# Host-specific 'hdfs-2.4.1' configuration
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# Customize Peel configuration values appearing in
#
#   https://github.com/stratosphere/peel/blob/master/peel-extensions/src/main/resources/reference.hdfs-2.4.1.conf
#
# here.
#

system {
    hadoop-2 {
        user = ${system.default.user}
        group = ${system.default.group}
        path {
            archive.dst = ${app.path.systems}
            archive.url = "https://archive.apache.org/dist/hadoop/core/hadoop-2.7.1/hadoop-2.7.1.tar.gz"
            archive.md5 = "203E5B4DAF1C5658C3386A32C4BE5531"
            archive.src = ${app.path.downloads}"/hadoop-2.7.1.tar.gz"
            home = ${system.hadoop-2.path.archive.dst}"/hadoop-2.7.1"
            config = ${system.hadoop-2.path.home}"/etc/hadoop"
            log = ${system.hadoop-2.path.home}"/logs"
            input = ${system.hadoop-2.config.core.fs.default.name}"/tmp/input"
            output = ${system.hadoop-2.config.core.fs.default.name}"/tmp/output"
        }
        format = false
        startup {
            max.attempts = ${system.default.startup.max.attempts}
            polling {
                counter = ${system.default.startup.polling.counter}
                interval = ${system.default.startup.polling.interval}
            }
        }
        config {
            # put list of masters
            masters = ${system.default.config.masters}
            # put list of slaves
            slaves = ${system.default.config.slaves}
            # hadoop-env.sh entries
            env {
                JAVA_HOME = ${system.default.config.java.home}
                HADOOP_INSTALL = ${system.hadoop-2.path.home}
                # directory where process IDs are stored
                HADOOP_PID_DIR = "/tmp/hadoop-2-stable/pid"
                # avoids loading wrong native library in the default case
                # override with /lib/native if lib exists
                HADOOP_COMMON_LIB_NATIVE_DIR = "$HADOOP_INSTALL/lib/native"
            }
            # core-site.xml entries
            core {
                fs.default.name = "hdfs://"${runtime.hostname}":44000/"
                io.file.buffer.size = 524288
                hadoop.tmp.dir = "/data/hadoop/peel/running/hadoop-2-stable/hadoop-2/tmp"
                hadoop.http.staticuser.user = "hdfs"

                # enable this if you want to use hadoop with native libraries
                # only use if there is a hadoop version compiled with native libraries for your environment!

                # enable client short circuit read
                # dfs.client.read.shortcircuit = true
                # dfs.domain.socket.path = "/data/hadoop/peel/running/hadoop-2-stable/hadoop-2/scr"
            }
            # hdfs-site.xml entries
            hdfs {
                # folders for namenode, data, and checkpoint storage (node-local)
                dfs.namenode.name.dir = "/data/hadoop/peel/running/hadoop-2-stable/namenode"
                dfs.datanode.data.dir = "/data/1/hadoop/peel/hadoop-2-stable/hadoop-2/datanode,/data/2/hadoop/peel/hadoop-2-stable/hadoop-2/datanode,/data/3/hadoop/peel/hadoop-2-stable/hadoop-2/datanode,/data/4/hadoop/peel/hadoop-2-stable/hadoop-2/datanode"
                dfs.namenode.checkpoint.dir = "/data/1/hadoop/peel/hadoop-2-stable/hadoop-2/check,/data/2/hadoop/peel/hadoop-2-stable/hadoop-2/check,/data/3/hadoop/peel/hadoop-2-stable/hadoop-2/check,/data/4/hadoop/peel/hadoop-2-stable/hadoop-2/check"
                dfs.replication = 2
                dfs.permissions.enabled = true
                dfs.blocksize = 134217728
                # namenode
                dfs.namenode.http-address = "0.0.0.0:44010"
                dfs.namenode.https-address = "0.0.0.0:44011"
                dfs.namenode.secondary.http-address = "0.0.0.0:44012"
                dfs.namenode.secondary.https-address = "0.0.0.0:44013"
                dfs.namenode.backup.address = "0.0.0.0:44014"
                dfs.namenode.backup.http-address = "0.0.0.0:44015"
                dfs.namenode.safemode.threshold-pct = "0.9f"
                dfs.namenode.safemode.extension = 10000
                dfs.namenode.handler.count = 100
                # datanode
                dfs.datanode.address = "0.0.0.0:44020"
                dfs.datanode.http.address = "0.0.0.0:44021"
                dfs.datanode.ipc.address = "0.0.0.0:44022"
                dfs.datanode.balance.bandwidthPerSec = 10000000000
                # journalnode
                dfs.journalnode.rpc-address = "0.0.0.0:44030"
                dfs.journalnode.http-address = "0.0.0.0:44031"
                dfs.journalnode.https-address = "0.0.0.0:44032"
            }

            # enable this if you want to use hadoop with native libraries
            # only use if there is a hadoop version compiled with native libraries for your environment!

            # env {
            #    HADOOP_COMMON_LIB_NATIVE_DIR = "$HADOOP_INSTALL/lib/native"
            #    HADOOP_OPTS= "-Djava.library.path="${system.hadoop-2.path.home}"/lib/native"
            # }
        }
    }
}
